{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyODjTtrn2OcqIiDW8BkKAg1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Get Visual Output of Model Inference\n","\n","`model-validation.ipynb` takes a video as input and runs inference on it. There are 2 possible ways of handling it:\n","\n","1. With Post Processing: The detections are made in the first phase and saved to a `json` file. IDs, coordinates, and detection confidence are recorded. There is also a dictionary that handles the times each ID is encountered. If the ID count is less than `min_count`, those detections will not be added to the final video.\n","\n","2. Without Post Processing: Default Ultralytics YOLO functions handle everything from inference to final output video creation."],"metadata":{"id":"Dem4hs8idM9Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-MKlzqYUk4h"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install ultralytics opencv-python filterpy ffmpeg supervision tqdm\n","!pip uninstall -y wandb\n","!pip install -U lap"],"metadata":{"id":"suKSLZAmWrNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace IDs with desired video inputs\n","!gdown 1qXLrFWWfO-s4ZUqpEoQwJA1tjU1kOIXw # Stationary"],"metadata":{"id":"bRfPN1NnVOFD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import sys\n","import json\n","import torch\n","import locale\n","import numpy as np\n","\n","from ultralytics import YOLO\n","os.environ[\"WANDB_DISABLED\"] = \"True\"\n","locale.getpreferredencoding = lambda: \"UTF-8\""],"metadata":{"id":"JpaO1H_BWlWA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace weight file with custom version or use default weights\n","model = YOLO(\"18042025.pt\")"],"metadata":{"id":"kKu-jTPuW_DP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace source path with custom file\n","source = \"/content/Stationary.mp4\""],"metadata":{"id":"XgJ3AvkTXJST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Post Processing"],"metadata":{"id":"7HvRJP8_ZV2a"}},{"cell_type":"code","source":["total_detections = {}\n","total_detections_count = {}\n","\n","# Gathering phase - For each frame, save results & update separate object count\n","with open(\"data.json\", \"w\", encoding = \"utf-8\") as f:\n","  for i, result in enumerate(model.track(source=SOURCE, stream=True, tracker=\"bytetrack.yaml\", conf=0.3)):\n","    frame_info = {\"index\" : i, \"detections\" : []}\n","    for box in result.boxes:\n","        if box.id != -1 and box.id is not None:\n","          arr = box.xyxy[0].tolist()\n","          frame_info[\"detections\"].append(\n","              {\n","                  \"id\" : int(box.id),\n","                  \"conf\" : float(box.conf),\n","                  \"xyxy\" : arr\n","              }\n","          )\n","          total_detections_count[int(box.id)] = total_detections_count.get(int(box.id), 0) + 1\n","\n","    json.dump(frame_info, f, ensure_ascii = False)\n","    f.write(\"\\n\")"],"metadata":{"id":"mkDyxEo-XWG-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adapted from https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n","\n","vid = cv2.VideoCapture(source)\n","\n","# Shows how many objects were filtered at the end\n","filtered_no = 0\n","\n","# Filtering threshold. Objects that appear for less than 5 frames are not displayed in the final output video\n","min_count = 5\n","\n","fps = vid.get(cv2.CAP_PROP_FPS)\n","width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","out = cv2.VideoWriter('output-stationary.avi', fourcc, fps, (width,  height))\n","\n","frame_index = 0\n","with open(\"data.json\", \"r\", encoding = \"utf-8\") as f:\n","  for line in f:\n","    print(f\"PROCESSING FRAME {frame_index}\")\n","    try:\n","      frame_info = json.loads(line)\n","    except Exception as e:\n","      print(\"JSON ERROR:\", e)\n","\n","    ret, frame = vid.read()\n","\n","    if not ret:\n","      print(\"ERROR receiving frame (stream end?). Exiting ...\")\n","      break\n","\n","    # If checks pass, this section draws the detections for a frame on said frame\n","    # while including details such as ID and confidence\n","    if frame_info['index'] == frame_index:\n","      for detection in frame_info['detections']:\n","        if total_detections_count.get(detection['id']) >= min_count:\n","          filtered_no += 1\n","          x1, y1, x2, y2 = map(int, detection['xyxy'])\n","          label = f\"ID: {detection['id']} Conf: {detection['conf']:.2f}\"\n","          cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 127, 0), 2)\n","\n","          # Text might flow out of the frame\n","          try:\n","            cv2.rectangle(frame, (x1, y1), (x1 + 150, y1 + 40), (255, 127, 0), cv2.FILLED) # Draw background for text\n","            cv2.putText(frame, label, (x1 + 5, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","          except Exception as e:\n","            print(\"ERROR WITH LABEL GENERATION:\", e)\n","\n","    out.write(frame)\n","    frame_index += 1\n","\n","vid.release()\n","out.release()\n","\n","print(f\"TOTAL NUMBER OF DETECTIONS: {sum(total_detections_count.values())}\")\n","print(f\"FILTERED NUMBER OF DETECTIONS: {filtered_no}\")"],"metadata":{"id":"McVwu0UcX1bW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move output file to a secure location so it does not disappear when the notebook runtime ends\n","!mv /content/output-stationary.avi /content/drive/MyDrive/"],"metadata":{"id":"94UgjLQzZJ1N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. No Post Processing"],"metadata":{"id":"Ox2q3Q8rZmba"}},{"cell_type":"code","source":["# Loop forces .track() function to compute results\n","for i, result in enumerate(model.track(\n","    source = source,\n","    stream = True,\n","    save = True,\n","    show = False,\n","    tracker = \"bytetrack.yaml\",\n","    conf = 0.3\n","    )):\n","  pass"],"metadata":{"id":"FBScsBlmZjf1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move output file to a secure location so it does not disappear when the notebook runtime ends\n","!mv /content/runs/detect/track/Stationary.avi /content/drive/MyDrive/"],"metadata":{"id":"pUXpBYHRZqNv"},"execution_count":null,"outputs":[]}]}